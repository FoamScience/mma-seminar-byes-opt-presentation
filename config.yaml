defaults:
  - meta: config.yaml
  - default_styling: config.yaml
  - _self_

version: 0.0.4

scene: 2D

slides:

  - title: We'll look at...
    number: "   "
    content:
      - type: items
        bullets:
          - "Bayesian Optimization: Benefits and Challenges"
          - Bayesian Optimization Workflow - a No-Equations approach
          - "foamBO: Bayesian Optimization on OpenFOAM cases"
          - "foamBO: Some use cases"
          - "Extreme Case: Dealing with huge search spaces"
        distance: 4
        anchor: "{{ title }}"
      - type: text
        text: "Focusing on..."
        next_to:
          target: "{{ last }}"
          dir: 2*DOWN
        align_to:
          target: "{{ title }}"
          dir: LEFT
        no_next_slide: false
      - type: items
        bullets:
          - foamBO as a means to streamline OpenFOAM case parametrization
          - Hyperparameters for strategy insights
        distance: 2
        anchor: "{{ last }}"

  - title: "Bayesian Optimization: Benefits and Challenges"
    number: "1❱"
    content:
      - type: items
        bullets:
          - Faster convergence, less expensive function evaluations
          - Can handle somewhat large search spaces efficiently
          - Can handle noisy/stochastic objective functions well enough
          - End up with surrogate models, with uncertainty estimates!
          - Convergence metrics based on probability of improvement
        distance: 4
        anchor: "{{ title }}"
      - type: items
        bullets:
          - "↓ Easy to get stuck at local optima!"
          - "↓ Cost for updating surrogates can be too high for high-dimentional problems"
        distance: 15
        anchor: "{{ title }}"
        colors:
          - text: "↓"
            color: self.secondary_color
      - type: reset
      - type: items
        bullets:
          - Acquisition functions balance exploration and exploitation
          - Eg. Expected Improvement (EI) -> max. expected improvement over best observation
          - Eg. Knowledge Gradient (KG) -> very expensive OFs, gain max information
        distance: 4
        anchor: "{{ title }}"
      - type: items
        bullets:
          - Assuming? OF values are samples from  a Gaussian Process
          - "Gaussian process: connect dots smoothly from know data points"
          - "'smoothly' as in: can be modeled with a normal distribution"
          - Can also switch to random forests, ... etc
        distance: 2
        anchor: "{{ last }}"

  - title: Bayesian Optimization Workflow
    number: "2❱"
    hook:
      name: bayes_opt
      filename: hooks/bayes_opt.py
      functions: bayes_opt_step
      num_samples: 3
      acquisition_function: "ei" # or 'ucb' or 'poi'
      kappa: 5 # default: 2.576, higher favores least explored spaces 
      n_iters: 20
      benchmark:
        type: text
        text: foamBO configuration for such a problem
        next_to:
          target: "{{ title }}"
          dir: 2*DOWN
        align_to:
          target: "{{ title }}"
          dir: LEFT
        no_next_slide: true
      parameters:
        type: code
        language: yaml
        line_numbers: false
        code: |
          parameters:
            x:
              type: range
              value_type: float
              bounds: [-200, 200]
              log_scale: False
          scopes:
            "/FxDict":
              x: "x"
        to_edge: LEFT
        shift: 0.5*DOWN
      objectives:
        type: code
        language: yaml
        line_numbers: false
        code: |
          objectives:
            F1:
              mode: 'shell'
              # command must write a scalar to stdout
              command: './F --F F1 --k 1 --m 0 --lb 0.01'
              # try to focus trials around this value
              threshold: 0
              minimize: True
              lower_is_better: True
        to_edge: RIGHT
        shift: 0.5*DOWN

  - title: "foamBO: Bayesian Optimization on OpenFOAM cases"
    number: "3❱"
    hook:
      name: foam_bo
      filename: hooks/foam_bo.py
      functions: foam_bo_step
      pareto:
        type: items
        bullets:
          - Pareto-frontier points
          - Optimize conflicting objectives
          - Acquisition functions are important
        distance: 4
        anchor: "{{ title }}"
      feature_importance:
        type: items
        bullets:
          - Which parameters are most important?
          - To which objectives?
        distance: 11
        anchor: "{{ title }}"

  - title: "foamBO: Some use cases"
    number: "4❱"
    content:
      - type: text
        text: "Sandia Flame D"
        next_to:
          target: "{{ title }}"
          dir: 4*DOWN
        align_to:
          target: "{{ title }}"
          dir: LEFT
        no_next_slide: true
      - type: items
        bullets:
          - Can we improve on default settings for the OpenFOAM tutorial?
          - minimal (CH4, CO2, T, U) errors to theory and execution time as objectives
          - searching for better turbulence models, chemistry and combustion mechanisms
        distance: 2
        anchor: "{{ last }}"
      - type: items
        bullets:
          - "Link: https://foamscience.github.io/SandiaD-LTS-Bayesian-Optimization"
          - "Convergence: ~30 trials, Search space size: 180"
        distance: 2
        anchor: "{{ last }}"
        colors:
          - text: "https://foamscience.github.io/SandiaD-LTS-Bayesian-Optimization"
            color: self.main_color
        align_to:
          target: "{{ title }}"
          dir: LEFT
      - type: reset
      - type: text
        text: "Annular Thermal Mixer"
        next_to:
          target: "{{ title }}"
          dir: 4*DOWN
        align_to:
          target: "{{ title }}"
          dir: LEFT
        no_next_slide: true
      - type: items
        bullets:
          - Can we improve on blade design of the OpenFOAM tutorial?
          - Power consumption, mixing quality, and blade durability for objectives
          - searching for optimal blade number and geometrical properties
        distance: 2
        anchor: "{{ last }}"
      - type: items
        bullets:
          - "Link: https://github.com/FoamScience/mixer-bayesian-optimization"
          - "Convergence: ~70 trials, Search space size: ~14.7k"
        distance: 2
        anchor: "{{ last }}"
        colors:
          - text: "https://github.com/FoamScience/mixer-bayesian-optimization"
            color: self.main_color
        align_to:
          target: "{{ title }}"
          dir: LEFT

  - title: "Extreme Case: Dealing with huge search spaces"
    number: "5❱"
    hook:
      name: brotato
      filename: hooks/brotato.py
      functions: brotato_step
      icon:
        type: image 
        image: brotato.png
        scale: 0.8
        to_edge: RIGHT
        no_next_slide: true
      description:
        type: items
        bullets:
          - A "strategy" game, have to survive 20 waves of enemies
          - At end of each wave, a shop presents items to buy
          - Level upgrades improve specific stats
        distance: 4
        anchor: "{{ title }}"
        align_to:
          target: "{{ title }}"
          dir: LEFT
      objectives:
        type: items
        bullets:
          - Find best strategy to win? -> too random, GP wouldn't work
          - Make sure the game is balanced -> winnable with character traits
        distance: 2
        anchor: "{{ last }}"
        align_to:
          target: "{{ title }}"
          dir: LEFT
      conventions:
        type: items
        bullets:
          - Mod the game to auto-play, and auto-buy items
          - Optimize with minimal game/code knowledge
          - How to setup optimization parameters? -> strategy modeling
        distance: 2
        anchor: "{{ last }}"
        align_to:
          target: "{{ title }}"
          dir: LEFT
      parameters:
        type: text
        text: "Parameter space analysis"
        next_to:
          target: "{{ title }}"
          dir: 4*DOWN
        align_to:
          target: "{{ title }}"
          dir: LEFT
        no_next_slide: true
      parameter_analysis:
        type: items
        bullets:
          - We fix the character, starting weapon and game difficulty
          - Item buying action weighs desire, stats category, and weapon synergies
          - Choosing upgrades is similar
          - Incorporated 16 character stats, each has a desire strategy
          - A few other parameters like shop rerolls are also modelled with desire functions
          - This ends up with an astronomical search space, a staggering 5e+118 for its size
        weights:
          - text: "staggering 5.e+118"
            weight: BOLD
        distance: 2
        anchor: "{{ last }}"
        align_to:
          target: "{{ title }}"
          dir: LEFT
      bro_objectives:
        type: text
        text: "Objective functions"
        next_to:
          target: "{{ title }}"
          dir: 4*DOWN
        align_to:
          target: "{{ title }}"
          dir: LEFT
        no_next_slide: true
      bro_objective_analysis:
        type: items
        bullets:
          - Minimize waves remaining -> expand effort to win the game
          - Maximize avg. survival stats -> protect against unkillable characters
          - Maximize avg. economy stats -> protect against infinite money
          - Maximize avg. damage stats -> protect against over powered characters
        distance: 2
        anchor: "{{ last }}"
        colors:
          - text: "->"
            color: self.main_color
        align_to:
          target: "{{ title }}"
          dir: LEFT
      bro_foambo:
        type: items
        bullets:
          - Optimization carried out with foamBO, 1 trial -> ~20 mins
          - Max parallel trials 5, but later gets capped at 3, for max. 100 trials
          - Converge if probability of improvement falls under 5%
          - Converges in 75 trials
        distance: 2
        weights:
          - text: "75 trials"
            weight: BOLD
        anchor: "{{ last }}"
        align_to:
          target: "{{ title }}"
          dir: LEFT
      objective_results:
        type: items
        bullets:
          - Pareto frontier reports remaining waves to be in [0-2]
          - Damage, Economy and Survival avg. stats seem reasonable
          - Eg. 91, 99, 46 respectively
          - Almost all stats are between 1.2% and 1.6% important for each metric
          - upgrade reroll aggressiveness is 3% important to economy stats
        distance: 6
        anchor: "{{ last }}"
        align_to:
          target: "{{ title }}"
          dir: LEFT
